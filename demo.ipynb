{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1a2733",
   "metadata": {},
   "source": [
    "\n",
    "# 🔎 POLARIS: Policy-Aligned Local Review Inference System\n",
    "Online reviews shape how people perceive local businesses and locations. Yet, irrelevant, misleading or low-quality reviews can distort reputation and mislead users.\n",
    "\n",
    "This project, developed for a hackathon challenge, lets you use your Ollama-based review evaluator directly in Jupyter, either for **single reviews** or **batch-process a CSV** to evaluate the quality and relevancy of online location reviews, aligning them with platform policies.\n",
    "\n",
    "**What it does**\n",
    "- Classifies each review as **Valid** or **Flagged** (sentiment-neutral).\n",
    "- If flagged, selects a **single strongest** policy violation.\n",
    "- Adds a short explanation.\n",
    "\n",
    "**Policies enforced**\n",
    "1. **No Advertisement** — no promotional content, discounts, links.\n",
    "2. **No Irrelevant Content** — must focus on the experience at the specified location.\n",
    "3. **No Rant Without Visit** — must be based on a real visit/experience (not hearsay).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Prereqs (run outside Jupyter or in a separate terminal)\n",
    "1) Install **Ollama**: https://ollama.ai/download  \n",
    "2) Pull a model (this notebook assumes `llama3.2`):  \n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "3) Start the Ollama server (in a terminal, not in this notebook if possible):\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "(Default endpoint: `http://localhost:11434`)\n",
    "\n",
    "> You *can* try starting it here with `!ollama serve &`, but it's more reliable to run it in a separate terminal so it keeps running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982258f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed: install Python packages used in this notebook\n",
    "# (Run once, then restart the kernel if prompted)\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install langchain-ollama langchain-core requests pandas ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7554b",
   "metadata": {},
   "source": [
    "\n",
    "## Imports & Model Setup\n",
    "This uses the same logic as your script, adapted for notebook use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d8aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete. If your Ollama server is running and the 'llama3.2' model is available, you're ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# -------------------------\n",
    "# Model & Prompt\n",
    "# -------------------------\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert in identifying trustworthy and policy-compliant location reviews.\n",
    "\n",
    "Location Being Reviewed:\n",
    "{location}\n",
    "\n",
    "Review to Evaluate:\n",
    "{review}\n",
    "\n",
    "Policies to Enforce (sentiment-neutral):\n",
    "    1. No Advertisement: Reviews should not contain promotional content, discount offers, or links.\n",
    "    2. No Irrelevant Content: Reviews should focus on the experience at the specified location, not unrelated matters.\n",
    "    3. No Rant Without Visit: Complaints or praise must be based on an actual visit/experience; pure speculation, hearsay, or second-hand rants should be flagged.\n",
    "\n",
    "Important: Negative or strongly critical reviews are allowed and should not be flagged solely due to sentiment. Only flag if a policy is violated.\n",
    "\n",
    "Instructions:\n",
    "    • Determine whether the review is Valid or Flagged.\n",
    "    • If flagged, choose the single strongest policy violated (Primary Violation).\n",
    "    • Provide a brief explanation (1 to 2 sentences).\n",
    "\n",
    "Output Format:\n",
    "• Decision: Valid / Flagged\n",
    "• Primary Violation (if flagged): [Policy Name]\n",
    "• Explanation: [Short reasoning]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def _normalize(name: str) -> str:\n",
    "    return name.strip().lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "\n",
    "def _guess_columns(headers: List[str]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Try to auto-detect location/review columns using common names.\n",
    "    Returns (location_col, review_col) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    norm_map = {_normalize(h): h for h in headers}\n",
    "\n",
    "    # Common variants\n",
    "    loc_candidates = [\n",
    "        \"location\", \"place\", \"venue\", \"restaurant\", \"store\", \"hotel\", \"site\", \"spot\"\n",
    "    ]\n",
    "    rev_candidates = [\n",
    "        \"review\", \"text\", \"comment\", \"feedback\", \"content\", \"body\", \"ratingtext\"\n",
    "    ]\n",
    "\n",
    "    found_loc = next((norm_map[n] for n in loc_candidates if n in norm_map), None)\n",
    "    found_rev = next((norm_map[n] for n in rev_candidates if n in norm_map), None)\n",
    "    return found_loc, found_rev\n",
    "\n",
    "def _parse_model_output(output_text: str):\n",
    "    \"\"\"\n",
    "    Best-effort parser for the model's three-line output format.\n",
    "    Returns dict with keys: decision, violation, explanation, raw\n",
    "    \"\"\"\n",
    "    lines = [l.strip() for l in output_text.splitlines() if l.strip()]\n",
    "    decision, violation, explanation = \"\", \"\", \"\"\n",
    "\n",
    "    for ln in lines:\n",
    "        low = ln.lower()\n",
    "        if \"decision:\" in low and not decision:\n",
    "            decision = ln.split(\":\", 1)[1].strip()\n",
    "        elif \"primary violation\" in low and not violation:\n",
    "            violation = ln.split(\":\", 1)[1].strip()\n",
    "        elif \"explanation:\" in low and not explanation:\n",
    "            explanation = ln.split(\":\", 1)[1].strip()\n",
    "\n",
    "    return {\n",
    "        \"Decision\": decision,\n",
    "        \"Primary Violation\": violation,\n",
    "        \"Explanation\": explanation,\n",
    "        \"RawOutput\": output_text,\n",
    "    }\n",
    "\n",
    "def evaluate(location: str, review: str):\n",
    "    \"\"\"Evaluate one review (returns a dict)\"\"\"\n",
    "    res = chain.invoke({\"location\": location.strip(), \"review\": review.strip()})\n",
    "    text = getattr(res, \"content\", res) if not isinstance(res, str) else res\n",
    "    return _parse_model_output(text)\n",
    "\n",
    "def evaluate_dataframe(df: pd.DataFrame, location_col: Optional[str] = None, review_col: Optional[str] = None, include_raw: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Notebook-friendly batch evaluation.\n",
    "    - Auto-detects columns if not provided.\n",
    "    - Returns a copy of the DataFrame with appended columns.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "\n",
    "    headers = list(df.columns)\n",
    "    loc_col, rev_col = location_col, review_col\n",
    "\n",
    "    if not loc_col or not rev_col:\n",
    "        guessed_loc, guessed_rev = _guess_columns(headers)\n",
    "        loc_col = loc_col or guessed_loc\n",
    "        rev_col = rev_col or guessed_rev\n",
    "\n",
    "    if not loc_col or not rev_col or loc_col not in headers or rev_col not in headers:\n",
    "        raise ValueError(f\"Could not find location/review columns. Available headers: {headers}\\n\"\n",
    "                         f\"Pass explicit column names via location_col=... and review_col=...\" )\n",
    "\n",
    "    out_df = df.copy()\n",
    "    decisions, violations, explanations, raws = [], [], [], []\n",
    "\n",
    "    for _, row in out_df.iterrows():\n",
    "        location = str(row.get(loc_col, \"\") or \"\").strip()\n",
    "        review = str(row.get(rev_col, \"\") or \"\").strip()\n",
    "        if not review:\n",
    "            decisions.append(\"\")\n",
    "            violations.append(\"\")\n",
    "            explanations.append(\"\")\n",
    "            raws.append(\"\")\n",
    "            continue\n",
    "        result = evaluate(location, review)\n",
    "        decisions.append(result[\"Decision\"])\n",
    "        violations.append(result[\"Primary Violation\"])\n",
    "        explanations.append(result[\"Explanation\"])\n",
    "        raws.append(result[\"RawOutput\"])\n",
    "\n",
    "    out_df[\"Decision\"] = decisions\n",
    "    out_df[\"Primary Violation\"] = violations\n",
    "    out_df[\"Explanation\"] = explanations\n",
    "    if include_raw:\n",
    "        out_df[\"RawOutput\"] = raws\n",
    "\n",
    "    return out_df\n",
    "\n",
    "def evaluate_csv_to_csv(input_path: str, output_path: Optional[str] = None, location_col: Optional[str] = None, review_col: Optional[str] = None, include_raw: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    File-based helper for batch processing (no interactive input()).\n",
    "    Returns path to the written CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(input_path):\n",
    "        raise FileNotFoundError(f\"File not found: {input_path}\")\n",
    "\n",
    "    df = pd.read_csv(input_path, encoding=\"utf-8-sig\")\n",
    "    out_df = evaluate_dataframe(df, location_col=location_col, review_col=review_col, include_raw=include_raw)\n",
    "\n",
    "    if not output_path:\n",
    "        base, ext = os.path.splitext(input_path)\n",
    "        output_path = f\"{base}_evaluated{ext}\"\n",
    "\n",
    "    out_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    return output_path\n",
    "\n",
    "print(\"✅ Setup complete. If your Ollama server is running and the 'llama3.2' model is available, you're ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1e90c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Notes & Tips\n",
    "- If you see connection errors, make sure **Ollama server is running** and the **model is pulled**.\n",
    "- You can swap models by changing `OllamaLLM(model=\"llama3.2\")` to any local model you've pulled.\n",
    "- To include the raw LLM output for debugging, set `include_raw=True` in the batch helpers.\n",
    "- When running large CSVs, expect it to take a while since each row calls the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6101c9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Evaluate a Single Review\n",
    "Run the next cell, edit the `location` and `review` strings, then run it again to test different inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5187af96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision': 'Valid',\n",
       " 'Primary Violation': 'None',\n",
       " 'Explanation': \"The review is focused on the experience at Cafe Aurora, mentioning a great cappuccino and relaxing atmosphere, without any promotional content or unrelated matters. There's no indication of speculation or hearsay.\",\n",
       " 'RawOutput': \"Decision: Valid\\nPrimary Violation: None\\nExplanation: The review is focused on the experience at Cafe Aurora, mentioning a great cappuccino and relaxing atmosphere, without any promotional content or unrelated matters. There's no indication of speculation or hearsay.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Single review evaluation. Feel free to enter your review and try!\n",
    "sample_location = \"Cafe Aurora\"\n",
    "sample_review = \"Great cappuccino and relaxing atmosphere!\"\n",
    "result = evaluate(sample_location, sample_review)\n",
    "result  # dict of Decision / Primary Violation / Explanation / RawOutput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5c764",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Batch Process a DataFrame (in-notebook)\n",
    "If you already have a `pandas.DataFrame`, use `evaluate_dataframe` and display the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f39c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>review</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Primary Violation</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pizza House</td>\n",
       "      <td>Best pizza! Visit www.pizzapromo.com for disco...</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>No Advertisement</td>\n",
       "      <td>The review contains a link to the website www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Furama Hotel</td>\n",
       "      <td>I had a weird dream yesterday</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>No Irrelevant Content</td>\n",
       "      <td>The review \"I had a weird dream yesterday\" is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sakura Sushi</td>\n",
       "      <td>Never been here, but I heard it’s terrible!</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>No Rant Without Visit</td>\n",
       "      <td>Although the review expresses a strongly criti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       location                                             review Decision  \\\n",
       "0   Pizza House  Best pizza! Visit www.pizzapromo.com for disco...  Flagged   \n",
       "1  Furama Hotel                      I had a weird dream yesterday  Flagged   \n",
       "2  Sakura Sushi        Never been here, but I heard it’s terrible!  Flagged   \n",
       "\n",
       "       Primary Violation                                        Explanation  \n",
       "0       No Advertisement  The review contains a link to the website www....  \n",
       "1  No Irrelevant Content  The review \"I had a weird dream yesterday\" is ...  \n",
       "2  No Rant Without Visit  Although the review expresses a strongly criti...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Build a small DataFrame and evaluate\n",
    "df_example = pd.DataFrame([\n",
    "    {\"location\": \"Pizza House\", \"review\": \"Best pizza! Visit www.pizzapromo.com for discounts!\"},\n",
    "    {\"location\": \"Furama Hotel\", \"review\": \"I had a weird dream yesterday\"},\n",
    "    {\"location\": \"Sakura Sushi\", \"review\": \"Never been here, but I heard it’s terrible!\"},\n",
    "])\n",
    "\n",
    "evaluated = evaluate_dataframe(df_example, include_raw=False)\n",
    "evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3acae7",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Batch Process a CSV (Path-based)\n",
    "- Your CSV should contain columns for location and review (auto-detects common names like `location`, `place`, `review`, `text`, etc.).  \n",
    "- If auto-detection fails, pass explicit column names.\n",
    "- You can download the sample csv file we used [here](https://github.com/edwin-ljx/Filtering-the-Noise-ML-for-Trustworthy-Location-Reviews/blob/main/location_reviews.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e99f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/scormon/Downloads/location_reviews_evaluated.csv\n"
     ]
    }
   ],
   "source": [
    "# Example: Batch process from a CSV path\n",
    "# 1) Put your CSV on disk and set the path below.\n",
    "# 2) Optionally set location_col and review_col if auto-detect doesn't work.\n",
    "\n",
    "input_csv_path = \"/Users/scormon/Downloads/location_reviews.csv\" # Input your file path \n",
    "output_csv_path = None  # or set a custom path\n",
    "out_path = evaluate_csv_to_csv(input_csv_path, output_path=output_csv_path, location_col=\"location\", review_col=\"review\", include_raw=False) # change accordingly\n",
    "print(\"Wrote:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc7ed7",
   "metadata": {},
   "source": [
    "\n",
    "## 4) **Evaluate Predictions vs. Ground Truth**  \n",
    "This section adds your evaluation script (decision/violation canonicalization, accuracy, and mismatch export) adapted for Jupyter.  \n",
    "Use it to compare model outputs (eg, from the batch step) against ground-truth labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f9d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Canonical maps (kept as data) ---\n",
    "_CANONICAL_DECISION_MAP = {\"valid\": \"Valid\", \"flagged\": \"Flagged\"}\n",
    "_CANONICAL_VIOLATION_MAP = {\n",
    "    \"no advertisement\": \"No Advertisement\",\n",
    "    \"no irrelevant content\": \"No Irrelevant Content\",\n",
    "    \"no rant without visit\": \"No Rant Without Visit\",\n",
    "    \"none\": \"None\",\n",
    "}\n",
    "# Optional: common aliases\n",
    "_VIOLATION_ALIASES = {\n",
    "    \"advertising\": \"No Advertisement\",\n",
    "    \"advertisement\": \"No Advertisement\",\n",
    "    \"ads\": \"No Advertisement\",\n",
    "    \"promo\": \"No Advertisement\",\n",
    "    \"promotion\": \"No Advertisement\",\n",
    "    \"irrelevant\": \"No Irrelevant Content\",\n",
    "    \"off-topic\": \"No Irrelevant Content\",\n",
    "    \"rant without visit\": \"No Rant Without Visit\",\n",
    "    \"speculation\": \"No Rant Without Visit\",\n",
    "    \"hearsay\": \"No Rant Without Visit\",\n",
    "    \"\": \"None\",\n",
    "}\n",
    "\n",
    "def _safe_strip(x) -> str:\n",
    "    return \"\" if x is None or (isinstance(x, float) and pd.isna(x)) else str(x).strip()\n",
    "\n",
    "def _norm(s: Optional[str]) -> str:\n",
    "    s = _safe_strip(s)\n",
    "    return \" \".join(s.lower().split())\n",
    "\n",
    "def canonicalize_decision(s: Optional[str]) -> str:\n",
    "    key = _norm(s)\n",
    "    if key.startswith(\"valid\"):\n",
    "        return \"Valid\"\n",
    "    if key.startswith(\"flag\"):\n",
    "        return \"Flagged\"\n",
    "    return _CANONICAL_DECISION_MAP.get(key, _safe_strip(s))\n",
    "\n",
    "def canonicalize_violation(s: Optional[str]) -> str:\n",
    "    key = _norm(s)\n",
    "    if key in _CANONICAL_VIOLATION_MAP:\n",
    "        return _CANONICAL_VIOLATION_MAP[key]\n",
    "    if key in _VIOLATION_ALIASES:\n",
    "        return _VIOLATION_ALIASES[key]\n",
    "    # fuzzy contains\n",
    "    if \"advert\" in key or \"promo\" in key:\n",
    "        return \"No Advertisement\"\n",
    "    if \"irrelev\" in key or \"offtopic\" in key:\n",
    "        return \"No Irrelevant Content\"\n",
    "    if \"visit\" in key or \"hearsay\" in key or \"speculat\" in key or \"rant\" in key:\n",
    "        return \"No Rant Without Visit\"\n",
    "    return \"None\" if key == \"\" else _safe_strip(s)\n",
    "\n",
    "def load_csv_rows(path: str) -> List[Dict]:\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "def evaluate_predictions(\n",
    "    rows: List[Dict],\n",
    "    pred_dec_col: str,\n",
    "    pred_vio_col: str,\n",
    "    gt_dec_col: str,\n",
    "    gt_vio_col: str,\n",
    "    id_col: Optional[str] = None,\n",
    "    mismatches_out: str = \"mismatches.csv\",\n",
    "    review_col_candidates: Optional[List[str]] = None,\n",
    "    preview_rows: int = 50,\n",
    "    show_preview: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate predictions vs ground truth and (optionally) show/save mismatches.\n",
    "    \"\"\"\n",
    "    review_col_candidates = review_col_candidates or [\n",
    "        \"review\", \"Review\", \"text\", \"Text\", \"content\", \"Content\"\n",
    "    ]\n",
    "\n",
    "    def _pick_review(r: Dict) -> str:\n",
    "        for c in review_col_candidates:\n",
    "            if c in r and _safe_strip(r.get(c)):\n",
    "                return _safe_strip(r.get(c))\n",
    "        return \"\"\n",
    "\n",
    "    total = len(rows)\n",
    "    correct_dec, correct_vio = 0, 0\n",
    "    mismatches: List[Dict[str, str]] = []\n",
    "\n",
    "    for r in rows:\n",
    "        pid = r.get(id_col, \"\") if id_col else \"\"\n",
    "\n",
    "        pred_dec = canonicalize_decision(r.get(pred_dec_col, \"\"))\n",
    "        gt_dec   = canonicalize_decision(r.get(gt_dec_col, \"\"))\n",
    "\n",
    "        pred_vio = canonicalize_violation(r.get(pred_vio_col, \"\"))\n",
    "        gt_vio   = canonicalize_violation(r.get(gt_vio_col, \"\"))\n",
    "\n",
    "        # If decision is Valid, violation must be None\n",
    "        if pred_dec == \"Valid\":\n",
    "            pred_vio = \"None\"\n",
    "        if gt_dec == \"Valid\":\n",
    "            gt_vio = \"None\"\n",
    "\n",
    "        if pred_dec == gt_dec:\n",
    "            correct_dec += 1\n",
    "        if pred_vio == gt_vio:\n",
    "            correct_vio += 1\n",
    "        else:\n",
    "            mismatches.append({\n",
    "                \"id\": _safe_strip(pid),\n",
    "                \"pred_decision\": pred_dec,\n",
    "                \"gt_decision\": gt_dec,\n",
    "                \"pred_violation\": pred_vio,\n",
    "                \"gt_violation\": gt_vio,\n",
    "                \"review\": _pick_review(r),\n",
    "            })\n",
    "\n",
    "    # Metrics\n",
    "    decision_acc  = (correct_dec / total) if total else 0.0\n",
    "    violation_acc = (correct_vio / total) if total else 0.0\n",
    "\n",
    "    # Report\n",
    "    print(\"===== Evaluation Summary =====\")\n",
    "    print(f\"Total rows:           {total}\")\n",
    "    print(f\"Decision accuracy:    {correct_dec}/{total} = {decision_acc:.2%}\")\n",
    "    print(f\"Violation accuracy:   {correct_vio}/{total} = {violation_acc:.2%}\")\n",
    "\n",
    "    # Save & preview mismatches\n",
    "    mismatches_df = pd.DataFrame(mismatches)\n",
    "    if not mismatches_df.empty:\n",
    "        mismatches_df.to_csv(mismatches_out, index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\n⚠️  Found {len(mismatches)} mismatches. Saved to: {mismatches_out}\")\n",
    "        if show_preview:\n",
    "            print(f\"📋 Preview (first {preview_rows} rows):\")\n",
    "            display(mismatches_df.head(preview_rows))\n",
    "    else:\n",
    "        print(\"\\n✅ No mismatches found.\")\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"decision_accuracy\": (correct_dec, total, decision_acc),\n",
    "        \"violation_accuracy\": (correct_vio, total, violation_acc),\n",
    "        \"mismatches_path\": mismatches_out if not mismatches_df.empty else None,\n",
    "        \"mismatches_df\": mismatches_df if not mismatches_df.empty else pd.DataFrame(\n",
    "            columns=[\"id\",\"pred_decision\",\"gt_decision\",\"pred_violation\",\"gt_violation\",\"review\"]\n",
    "        ),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8e681",
   "metadata": {},
   "source": [
    "\n",
    "### Run an evaluation on a CSV\n",
    "1. Set `eval_csv_path` to your CSV file that contains predictions and ground truth.  \n",
    "2. Adjust column names below if yours differ from the defaults:\n",
    "   - Predictions: `Decision`, `Primary Violation`\n",
    "   - Ground Truth: `GT_Decision`, `GT_Violation`\n",
    "3. (Optional) Provide an ID column name if available.\n",
    "4. Run the cell.\n",
    "\n",
    "\n",
    "You can download the sample csv file we used [here](https://github.com/edwin-ljx/Filtering-the-Noise-ML-for-Trustworthy-Location-Reviews/blob/main/location_reviews_evaluated.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f29d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Evaluation Summary =====\n",
      "Total rows:           20\n",
      "Decision accuracy:    18/20 = 90.00%\n",
      "Violation accuracy:   14/20 = 70.00%\n",
      "\n",
      "⚠️  Found 6 mismatches. Saved to: mismatches.csv\n",
      "📋 Preview (first 50 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred_decision</th>\n",
       "      <th>gt_decision</th>\n",
       "      <th>pred_violation</th>\n",
       "      <th>gt_violation</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R006</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>No Rant Without Visit</td>\n",
       "      <td>No Irrelevant Content</td>\n",
       "      <td>I love my new phone’s camera—portrait mode is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XIC1</td>\n",
       "      <td>Valid</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>None</td>\n",
       "      <td>No Irrelevant Content</td>\n",
       "      <td>This place reminds me of my favorite video gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R018</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>No Advertisement</td>\n",
       "      <td>No Rant Without Visit</td>\n",
       "      <td>I’ve never brought my pet here, but prices loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R011</td>\n",
       "      <td>Valid</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>None</td>\n",
       "      <td>No Rant Without Visit</td>\n",
       "      <td>Never visited this place, but I’m giving 1 sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XIC2</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>No Rant Without Visit</td>\n",
       "      <td>No Irrelevant Content</td>\n",
       "      <td>I just bought new running shoes, can’t wait to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XIC4</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>Flagged</td>\n",
       "      <td>No Rant Without Visit</td>\n",
       "      <td>No Irrelevant Content</td>\n",
       "      <td>The stock market is crazy these days, thought ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id pred_decision gt_decision         pred_violation  \\\n",
       "0  R006       Flagged     Flagged  No Rant Without Visit   \n",
       "1  XIC1         Valid     Flagged                   None   \n",
       "2  R018       Flagged     Flagged       No Advertisement   \n",
       "3  R011         Valid     Flagged                   None   \n",
       "4  XIC2       Flagged     Flagged  No Rant Without Visit   \n",
       "5  XIC4       Flagged     Flagged  No Rant Without Visit   \n",
       "\n",
       "            gt_violation                                             review  \n",
       "0  No Irrelevant Content  I love my new phone’s camera—portrait mode is ...  \n",
       "1  No Irrelevant Content  This place reminds me of my favorite video gam...  \n",
       "2  No Rant Without Visit  I’ve never brought my pet here, but prices loo...  \n",
       "3  No Rant Without Visit  Never visited this place, but I’m giving 1 sta...  \n",
       "4  No Irrelevant Content  I just bought new running shoes, can’t wait to...  \n",
       "5  No Irrelevant Content  The stock market is crazy these days, thought ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'total': 20,\n",
       " 'decision_accuracy': (18, 20, 0.9),\n",
       " 'violation_accuracy': (14, 20, 0.7),\n",
       " 'mismatches_path': 'mismatches.csv',\n",
       " 'mismatches_df':      id pred_decision gt_decision         pred_violation  \\\n",
       " 0  R006       Flagged     Flagged  No Rant Without Visit   \n",
       " 1  XIC1         Valid     Flagged                   None   \n",
       " 2  R018       Flagged     Flagged       No Advertisement   \n",
       " 3  R011         Valid     Flagged                   None   \n",
       " 4  XIC2       Flagged     Flagged  No Rant Without Visit   \n",
       " 5  XIC4       Flagged     Flagged  No Rant Without Visit   \n",
       " \n",
       "             gt_violation                                             review  \n",
       " 0  No Irrelevant Content  I love my new phone’s camera—portrait mode is ...  \n",
       " 1  No Irrelevant Content  This place reminds me of my favorite video gam...  \n",
       " 2  No Rant Without Visit  I’ve never brought my pet here, but prices loo...  \n",
       " 3  No Rant Without Visit  Never visited this place, but I’m giving 1 sta...  \n",
       " 4  No Irrelevant Content  I just bought new running shoes, can’t wait to...  \n",
       " 5  No Irrelevant Content  The stock market is crazy these days, thought ...  }"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example (edit and run):\n",
    "eval_csv_path = \"/Users/scormon/Downloads/location_reviews_evaluated.csv\" # Input file path\n",
    "rows = load_csv_rows(eval_csv_path)\n",
    "evaluate_predictions(\n",
    "    rows,\n",
    "    pred_dec_col=\"Decision\",\n",
    "    pred_vio_col=\"Primary Violation\",\n",
    "    gt_dec_col=\"correct_decision\",\n",
    "    gt_vio_col=\"correct_labels\",\n",
    "    id_col=\"review_id\",  # or e.g. \"id\"\n",
    "    mismatches_out=\"mismatches.csv\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
